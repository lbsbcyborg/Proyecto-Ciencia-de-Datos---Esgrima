{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Initial Description Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La determinación de la postura corporal se realiza mediante la identificación y estimación de puntos corporales a través de modelos de aprendizaje de maquina. \n",
    "\n",
    "En este caso, usaremos los APIs de MediaPipe para estimar la postura corporal y las manos. Para la determinación de la postura corporal, MediaPipe Pose utiliza 33 puntos corporales ubicados en diferentes partes del cuerpo como lo vemos en la siguiente figura:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MediaPipe Pose](img/pose_tracking_full_body_landmarks.PNG)\n",
    "\n",
    "Fuente: MediaPipe Pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así mismo, para la estimación de las manos, usaremos los modelos de estimación de la mano de MediaPipe Hands para la estimación de los 21 puntos ubicados en la mano y sus coordenadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MediaPipe Hand](img/hand_landmarks.PNG)\n",
    "\n",
    "Fuente: MediaPipe Hands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la combinación de estos dos modelos nos ayuda a estimar por completo la postura corporal del sujeto en la imagen o el video junto con sus 2 manos con un total de 75 puntos estimados en el cuerpo humano.\n",
    "\n",
    "Cada punto identificado consiste de coordenadas X, Y y Z para los 33 puntos de la postura corporal como para los 21 puntos de cada mano, así como un valor de visibilidad que indica la probabilidad de que ese punto corporal este visible o no en la imagen (unicamente encontrado en los 33 puntos de la postura corporal). Por lo cual, el total de datos capturados en cada imagen son: (33 * 4) + (21 * 3) + (21 * 3) = 258"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "Para ver un ejemplo de como funciona la estimación de la postura corporal y las manos usando MediaPipe seguiremos la guia encontrada en el proyecto de GitHub de Nicholas Renotte \"Action Detection for Sign Language\"[1]. En la siguiente imagen, se puede ver los puntos corporales superiores y los de la mano izquierda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![landmarks_example](img/image-2.PNG)\n",
    "\n",
    "Fuente: Elaboración Propia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliografía\n",
    "\n",
    "1. MediaPipe Documentation. 2021 https://google.github.io/mediapipe/solutions/pose.html\n",
    "\n",
    "2. Nicholas Renotte. ActionDetectionforSignLanguage. 2021. https://github.com/nicknochnack/ActionDetectionforSignLanguage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
